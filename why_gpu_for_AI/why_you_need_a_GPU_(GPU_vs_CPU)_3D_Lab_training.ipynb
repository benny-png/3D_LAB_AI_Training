{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS66_mOiUnla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4190f7-5fc6-4beb-dc2e-33c316da5cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "CPU time: 213.0472 seconds\n",
            "GPU time: 3.7277 seconds\n",
            "GPU speedup: 57.15x faster\n",
            "\n",
            "Maximum difference between CPU and GPU results: 0.0068359375\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Matrix sizes\n",
        "size = 20000\n",
        "\n",
        "# Create random matrices\n",
        "matrix1 = torch.randn(size, size)\n",
        "matrix2 = torch.randn(size, size)\n",
        "\n",
        "# CPU computation\n",
        "start_time = time.time()\n",
        "cpu_result = torch.matmul(matrix1, matrix2)\n",
        "cpu_time = time.time() - start_time\n",
        "print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
        "\n",
        "# Move matrices to GPU\n",
        "if torch.cuda.is_available():\n",
        "    matrix1_gpu = matrix1.to(device)\n",
        "    matrix2_gpu = matrix2.to(device)\n",
        "\n",
        "    # GPU computation\n",
        "    start_time = time.time()\n",
        "    gpu_result = torch.matmul(matrix1_gpu, matrix2_gpu)\n",
        "    # Force GPU to finish computation\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start_time\n",
        "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
        "    print(f\"GPU speedup: {cpu_time/gpu_time:.2f}x faster\")\n",
        "\n",
        "    # Verify results match\n",
        "    difference = torch.max(torch.abs(cpu_result - gpu_result.cpu()))\n",
        "    print(f\"\\nMaximum difference between CPU and GPU results: {difference}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Hardware Acceleration: GPUs, NPUs, and CUDA ðŸ”¥\n",
        "## A Journey Through Parallel Computing History ðŸš€\n",
        "\n",
        "### Core Basics: CPU vs GPU ðŸ§ \n",
        "At the heart of every computer is the CPU (Central Processing Unit). Think of it as a brilliant manager capable of handling complex, varied tasks but only a few at a time. A CPU core contains sophisticated components:\n",
        "- Control Unit: The \"brain\" making decisions\n",
        "- ALU (Arithmetic Logic Unit): Handles calculations\n",
        "- Cache: Super-fast local memory\n",
        "- Various other components for task management\n",
        "\n",
        "GPUs (Graphics Processing Units) emerged with a different philosophy - sacrifice versatility for raw computational power ðŸ’ª. They achieve this by:\n",
        "- Maximizing ALUs (the calculation units)\n",
        "- Reducing control logic\n",
        "- Optimizing for parallel operations\n",
        "- Trading complex decision-making for throughput\n",
        "\n",
        "### The Graphics Era (1990s - Early 2000s) ðŸŽ®\n",
        "Initially, GPUs were single-purpose devices. Companies like NVIDIA and ATI (now part of AMD) built them specifically for rendering graphics in games and professional applications. The hardware was literally designed as a pipeline:\n",
        "1. Input geometry data\n",
        "2. Process vertices\n",
        "3. Rasterize to pixels\n",
        "4. Output to display\n",
        "\n",
        "Programming these early GPUs meant using graphics-specific languages like OpenGL and DirectX. Everything had to be expressed in terms of graphics operations, even if you wanted to do other types of calculations.\n",
        "\n",
        "### The GPGPU Revolution (2007-2008) âš¡\n",
        "A major breakthrough came when researchers realized GPUs' massive parallel processing power could be used for non-graphics tasks. However, there was a problem: how do you use graphics hardware for general computing?\n",
        "\n",
        "NVIDIA's introduction of CUDA in 2007 was revolutionary. CUDA provided:\n",
        "- A way to program GPUs without pretending everything was a graphics problem\n",
        "- Tools for general-purpose computing\n",
        "- A foundation for scientific and deep learning applications\n",
        "\n",
        "### The Deep Learning Boom (2012 onwards) ðŸ“ˆ\n",
        "The real explosion in GPU computing came with deep learning. Alex Krizhevsky's AlexNet in 2012, implemented on GPUs, demonstrated unprecedented performance in image recognition. This sparked:\n",
        "- Widespread adoption of GPUs for AI training\n",
        "- Development of specialized libraries like cuDNN\n",
        "- Integration of AI-specific features into GPU hardware\n",
        "\n",
        "### Enter the NPU Era (2016 onwards) ðŸ¤–\n",
        "As AI workloads became more specific, companies began developing Neural Processing Units (NPUs) - chips designed exclusively for deep learning:\n",
        "- Google's TPU (2016): Custom-built for TensorFlow operations\n",
        "- Apple's Neural Engine (2017): Integrated into mobile devices\n",
        "- NVIDIA's Tensor Cores: Hybrid approach combining GPU flexibility with NPU-like features\n",
        "\n",
        "### Why GPUs Still Dominate AI ðŸ‘‘\n",
        "Despite specialized NPUs, GPUs remain central to deep learning because:\n",
        "1. Mature Software Ecosystem:\n",
        "   - CUDA's widespread adoption\n",
        "   - Extensive libraries and tools\n",
        "   - Large developer community\n",
        "\n",
        "2. Flexibility:\n",
        "   - Can handle graphics, computing, and AI\n",
        "   - Useful for development and research\n",
        "   - Cost-effective for organizations\n",
        "\n",
        "3. Continuous Evolution:\n",
        "   - Modern GPUs incorporate specialized AI features\n",
        "   - Regular architecture improvements\n",
        "   - Backward compatibility maintenance\n",
        "\n",
        "### Looking Forward ðŸ”®\n",
        "The hardware acceleration landscape continues to evolve:\n",
        "- More specialized AI accelerators\n",
        "- Hybrid architectures combining different approaches\n",
        "- Focus on energy efficiency\n",
        "- Edge computing optimization\n",
        "\n",
        "Understanding this history helps us appreciate why we use these tools today and where they might go tomorrow. The journey from graphics-only GPUs to today's AI accelerators shows how hardware adapts to meet new computational challenges! ðŸŒŸ"
      ],
      "metadata": {
        "id": "e8CKBFrJU-er"
      }
    }
  ]
}